# Meta-Learning for Few-Shot Land Cover Classification

# Abstract
The representations of the Earth’s surface vary from one geographic region to another. For instance, the appearance of urban areas differs between continents, and seasonality influences the appearance of vegetation. To capture the diversity within a single category, such as urban or vegetation, requires a large model capacity and, consequently, large datasets. In this work, we propose a different perspective and view this diversity as an inductive transfer learning problem where few data samples from one region allow a model to adapt to an unseen region. We evaluate the modelagnostic meta-learning (MAML) algorithm on classification and segmentation tasks using globally and regionally distributed datasets. We find that few-shot model adaptation outperforms pre-training with regular gradient descent and fine-tuning on the (1) Sen12MS dataset and (2) DeepGlobe dataset when the source domain and target domain differ. This indicates that model optimization with meta-learning may benefit tasks in the Earth sciences whose data show a high degree of diversity from region to region, while traditional gradient-based supervised learning remains suitable in the absence of a feature or label shift.

# Introduction
A growing constellation of satellites, combined with cloud computing and deep learning, offers an objective and scalable way to monitor global issues from deforestation and wildfires to urban development and road flooding. For many of these prediction problems, the bottleneck to making accurate and timely predictions has shifted away from satellite imagery availability or data processing limits and toward a lack of ground truth labels. At the same time, these tasks share characteristics in remotely sensed imagery — such as ground sampling distance, seasonality, and spectral characteristics — no matter where on Earth they are taken. This raises the question of whether prediction in label-scarce regions could be improved if each model were to benefit from knowledge contained in all the datasets, rather than solving the same prediction problem across different geographies or time slices with independent models trained on small disjoint datasets.

Figure 1: A principal component analysis (PCA) on VGG-16 features of cropland images from different countries. Representations of the same class vary geographically; applying models trained on one geography to another would violate the assumption in traditional supervised learning that train and test distributions are equal. Model-agnostic meta learning provides a framework for inductive transfer learning that adapts the model to a new region with few data samples.

The concept of using knowledge gained while solving one problem to aid the solving of another is known in machine learning as transfer learning. Transferring knowledge between tasks or domains is successful when the problems are different but related. We argue that the diverse nature of representations on the Earth’s surface is a prime example of different-but-related tasks. We illustrate this in Fig.1 using representations of cropland from four different countries. Croplands across the world are distinct from each other, yet they share characteristics. Transfer learning allows models to both adapt to each distribution individually and share knowledge across regions: countries like Angola and Mali, for which smaller labeled datasets are available, could then benefit from larger labeled datasets from countries like Brazil and Poland.

Thus far, transfer learning on remote sensing data has largely focused on fine-tuning pre-trained models and performing domain adaptation (Section 2). In this work, we explore meta-learning, in which models not only learn from data to perform tasks but learn how to learn to perform tasks through experiencing tasks on a variety of datasets. In particular, we use model-agnostic meta-learning (MAML) for the problem of inductive transfer-learning, where the generalization is induced by a few labeled examples in the target domain. A schematic of MAML is shown in Fig.2 and the algorithm is described in Section 3.2.

Our main contributions are (1) demonstrating that remote sensing tasks across geographies can be restructured as one meta-learning problem and (2) evaluating MAML for few-shot classification and segmentation of multi-spectral and high-resolution remote sensing images; specifically, the well-cited benchmark datasets Sen12MS and DeepGlobe.

# Related Work
Transfer learning can be divided into subcategories depending on the amount of labeled data available in the source and target domains. Our work is focused on the scenario in which ample labels exist in the source domain, but few exist in the target domain. We summarize the related remote sensing methodology accordingly.

In such a setting, one common transfer learning technique is pre-training a neural network on ImageNet and fine-tuning it on an application-specific dataset. For high-resolution remotely sensed imagery, these include airplane detection, high-resolution land cover classification, and disaster mapping. Xie et al. extended this concept by swapping ImageNet for the proxy task of night-light prediction that allowed them to estimate poverty in African regions with a limited number of labeled poverty data points. These approaches require a significant amount of problem design, such as the choice of proxy datasets or model and which parameters to fine-tune, and, thus, usually focus on a limited number of hand-selected tasks.

A second class of methods using deep learning for labelscarce tasks in remote sensing has focused on developing novel network architectures or loss functions to make learning more label-efficient. So far, these methods have focused on optical, SAR, and hyperspectral image classification. While they decrease the number of labels required for any optical, SAR, or hyperspectral task, these methods do not explicitly endeavor to transfer knowledge from a data-rich geography to a data-poor one.

Non-deep learning methods for domain adaptation were summarized by Tuia et al. and include selecting invariant features, adapting data distributions, and adapting classifiers via semi-supervised learning. For the most part, such methods generalize only across small regions rather than worldwide, while sometimes requiring a feature space in which inputs can be modeled as a mixture of Gaussians or some other predefined distribution.

Lastly, meta-learning is beginning to be explored for remote sensing applications. Alajaji and Alhichri describe preliminary results of MAML on few-shot UC Merced, OPTIMAL-31, and AID RS classification, though again not with a focus on cross-geography generalization.

# Meta-learning
Meta-learning considers a large number of related tasks to arrive at a predictive function that can perform well on unseen tasks τ after seeing a few data samples. Even though meta-learning has been a topic in machine learning for decades, it has recently gained popularity for few-shot problems and has been re-introduced under a “model agnostic” framework with rapid developments in the field.

## Terminology and Definitions
Meta-learning introduces a set of terms that may be new to some readers, so we clarify them in this section. A task τ is comprised of a support dataset Dsupport to adjust the model parameters to the specific task and a query dataset D query to evaluate the performance. Each dataset is comprised of inputs and corresponding labels from a data distribution. A k-shot, n-way classification task aims to distinguish between n classes and is trained on k examples per class. Each task is drawn from a distribution over tasks to yield a set of tasks. The meta-learner learns how to learn by training and evaluating on the metatraining set. Meta-learning hyperparameters are tuned on the meta-validation set. The meta-test set measures generalization on new, unseen tasks.

## Model-Agnostic Meta Learning (MAML)
Neural network parameters are usually initialized randomly and optimized iteratively via gradient descent to perform well on a single dataset, as shown in Algorithm 1. Model-agnostic meta-learning (MAML) extends gradient descent by optimizing for a model initialization θ that leads to good performance on a set of related tasks We contrast the regular gradient descent with the MAML optimization algorithm in Algorithms 1 and 2. Meta-training is divided into an inner loop and an outer loop. In the inner loop, networks initialized with θ are updated to each task via t steps of gradient descent on D support of each task. This results in models with parameters φi adapted to each task τi. The outer loop updates θ based on the performance of φi on Dquery of the


meta-training batch. In so doing, MAML requires secondorder gradient calculations. The algorithm looks for a better θ until convergence, upon which the generalization error is computed on unseen meta-test tasks.

# Datasets
We evaluate model-agnostic meta-learning on two public remote sensing datasets that cover optical and radar data at medium and very high resolution.

## 4.1. Sentinel-1/2
Multi-Spectral (Sen12MS) Dataset The Sentinel-1/2 Multi-Spectral (Sen12MS) dataset is a novel globally distributed satellite image classification and segmentation dataset. It contains 280662 Sentinel 2 (optical) and Sentinel 1 (radar) tiles from 125 distinct regions at four different seasons. The optical and radar images were resampled to 10 m ground sampling distance and span 256 -> 256 px in height and width. The original dataset uses tile-overlaps of 50%. For this work, we removed the overlap to ensure independence of support and query datasets, which yielded 200306 128 x 128 px tiles. We show true color examples and principal component embeddings on VGG-16 features of four distinct regions in Fig. 1. Each image tile is accompanied by a land cover label with a comparatively coarse resolution of 500 m from the MODIS Land Cover product MCD12Q1 V6 upsampled to 10 m. In this work, we use the Sen12MS dataset for classification and assign the most common pixel-level label to the image tile. We use the simplified label-scheme of International Geosphere Biosphere Programme (IGBP) categories with 10 distinct classes, consistent with the IEEE Data Fusion Contest 2020. In Fig.3a, the 125 globally distributed regions are shown separated into metatrain, meta-validation, and meta-test sets. Each region contains between 196 and 850 tiles with a region-specific class distribution. We also show an overview of all tiles of the region 131 (Marseille) from the summer season true-color and labels. The individual 128 ⇥ 128 px tiles are randomly assigned to the support or query partition of each region. The objective is to classify each tile with its most frequent label class. Figure 3b illustrates this on an example of a 2-shot 2-way task. In this case, task datasets Dquery τ and Dτ support contain k=2 randomly chosen tile-label pairs of n=2 distinct classes chosen from the available classes in the region.

## DeepGlobe Land Cover Segmentation Dataset
The DeepGlobe Challenge was introduced at CVPR 2018 to advance state-of-the-art satellite image analysis. Here, we used the land cover segmentation data to explore the use of MAML on high-resolution satellite imagery.

The DeepGlobe land cover segmentation dataset is comprised of very high resolution (0.5 m) DigitalGlobe Vivid+ images of dimension 2448 ⇥ 2448 px with three RGB channels. In total, there are 803 training images, each with human-annotated semantic segmentation labels covering seven land cover classes: urban, agriculture, rangeland, forest, water, barren, and unknown. For the competition, 171 validation images and 172 test images were also provided. However, since they do not have corresponding labels, we did not include them in the following experiments. Across the training images, the most common class is agriculture (58 % of pixels), followed by forest (11 %), urban (11 %), rangeland (8 %), barren (8 %), water (3 %), and unknown (0.05 %).

We divided the DeepGlobe training set into three metadatasets: a meta-training set on which to train MAML, a meta-validation set on which to tune MAML hyperparameters, and a meta-test set on which to evaluate generalization (Fig. 4a). Ideally, we would evaluate whether meta-learned models generalize better to new geographic regions. However, the DeepGlobe Land Cover dataset does not tag images with latitude and longitude. In the absence of geographic information, we split the images in two ways:
- At random, i.e. the 803 images were sampled uniformly at random into a 500-image meta-train, a 150- image meta-val, and a 153-image meta-test set.
- Using unsupervised clustering on features extracted from a pre-trained network. DeepGlobe images were fed into a VGG-16 network pre-trained on ImageNet, and for each image, a 4096-dimensional vector was extracted from the first layer in the classifier. We used k-means to assign the images into 6 clusters and the 6 clusters were divided at random into the meta-train, meta-val, and meta-test sets. The resulting datasets contained 454, 166, and 183 images, respectively. Figure 6a visualizes the distributions of image features for the meta-train, meta-val, and meta-test sets under these two splitting methodologies. The results across the two splits will illuminate the settings under which MAML improves upon pre-training and training from scratch.

Each image was further divided into 16 sub-images, each of dimension 612 ⇥ 612 px (Fig. 4b). Eight sub-images were placed in the support set and 8 in the query set. At meta-train time, k shots of 306 ⇥ 306 px tiles were sampled from the support set and q queries were sampled from the query set. At meta-test time, the entire query set was fed into the model as 32 tiles to compute metrics (Fig. 4c).

Put succinctly, our DeepGlobe experiments explore whether a model can learn to segment a large region (1.2 km -> 1.2 km) of high resolution satellite imagery after seeing only a small labeled section (153 m -> 153 m) of it.

# Models
Model-agnostic meta-learning is an optimization algorithm that uses gradient descent and can be employed for any neural network architecture. In this work, we chose two popular models for image classification and segmentation.

## CNN Classification Model
Following other meta-learning approaches, we used a straightforward CNN architecture for the Sen12MS classification objective. The network consisted of 7 stacked layers with 64 convolutional 3->3 px kernels followed by batch normalization, ReLU activation function, and max-pooling of size 2. The input tensor X2 R128×128×15 of joint Sentinel-2 and Sentinel-1 bands is projected to a 64-dimensional feature vector that maps to the output vector y for each of the classes.

## U-Net Segmentation Model
For the DeepGlobe segmentation task, we employed the popular U-Net architecture. It is a fully-convolutional segmentation model with skip connections between encoder and decoder. We used four downsampling and upsampling layers so that the input tensor is projected to a hidden representation, which is then added to intermediate hidden states from the encoder (skip connections) while being upsampled and convolved to an output tensor whereupon each pixel represents one class label.

# Experiments
We experimentally evaluated the classification and segmentation performance of deep learning models with the same architecture trained with regular gradient descent (pretrained) Algorithm 1 and MAML Algorithm 2.

Figure 5: Classification results on Sen12MS. Regular pre-training with gradient descent leads to good zero-shot performance, while models trained with the model-agnostic meta learning algorithms outperform regular pretraining and the randomly initialized baseline clearly throughout all ten seen examples from a unseen region.

## Sen12MS Classification
We assumed that data from the meta-train regions were readily available, but at most ten image-label pairs per class can be seen from the meta-test regions. This corresponds to a 4-way 10-shot classification scenario with four randomly selected classes from one region. It reflects use-case of interest to this work, where labeled data is available in some regions but not in others.

We trained the classification models with MAML on 4-way 2-shot datasets from the meta-train regions. We treated each sub-dataset D support and Dquery as a single batch of N = k x n = 8 samples.

**Baselines.** We compared the MAML-trained model with a model that was pre-trained on all available data from the meta-train regions using regular gradient descent Algorithm 1. We pre-trained this model with the same 4-way 2-shot batches as MAML but used the combined support and query sets for training. This resulted in a batch size of 16 image label pairs. Finally, we also considered the scenario of having no additional data from meta-train regions. Here, we initialized the model randomly without any prior training, and train on each task’s support set from scratch; we refer to this baseline as the random model.

**Evaluation.** With the three initial CNN model parameterizations, i.e. MAML-trained, pretrained, and random, we evaluated the ability to adapt to new unseen meta-test regions based on at most ten data samples. For this, we sampled 100 4-way 10-shot tasks from the meta-test regions. We fine-tuned the models on subsets of D support, while we report performance metrics on Dquery on all ten examples per class. The number of samples seen from Dsupport was varied incrementally from zero-shot to 10-shot. Zero-shot represents no fine-tuning and shows the performance that can be obtained solely based on data from the meta-train regions. Training on batches of 1-shot to 10-shot provides increasingly more data from the target region to the models. The meta-val regions were used to determine a suitable step size α and gradient steps on the same data batch n for fine-tuning the pre-trained model. We evaluated these hyperparameters via grid search for each shot independently.

**Classification Results.** In Section 6.1, we report the accuracy scores for an increasing number of shots. The zeroshot case, without any adaptation to the particular meta-test region, shows that the regular pre-trained model performed best with 55 % accuracy and a kappa score of 0.47. Without any adaptation on the target-region, MAML predictions are low in accuracy, which highlights a distinct difference between meta-learning and pre-training. However, when a single data sample from the meta-test region is provided (1-shot), the MAML-trained model (74 % accuracy, 0.68 kappa score) outperforms the pre-trained model (59 % accuracy, 0.51 kappa score) by a large margin. The pre-trained model only shows a comparatively slight increase in accuracy (54 % to 66 %) throughout all seen examples while the MAML-trained model scores 80 % accuracy and 0.76 kappa score with all 10 shots.

## DeepGlobe Land Cover Segmentation
Our second experiment demonstrates the use of MAML on the DeepGlobe land cover segmentation dataset. Each DeepGlobe image was considered its own task and we trained a U-Net via MAML to segment the query set of an image after being shown k shots from the support set. The experiments were designed to investigate the effect on the generalization of (1) meta-training label quantity (number of support and query sub-images), (2) meta-test label quantity (number of shots), and (3) distributional shift between meta-train and meta-test sets (random split versus clustered split of meta-datasets). The number of labeled sub-images in the support and query sets was varied to be m ∈ {1, 2, 4, 8} and the number of shots used to adapt the U-Net was in the range k ∈ {1, 2, 3, 4, 5}. Hyperparameters, such as the number of epochs to meta-train MAML or train a model from scratch, were selected using performance on the meta-validation set.

**Baselines.** Similar to the Sen12MS evaluation, we compared MAML to two baselines: (1) a U-Net pre-trained on the meta-training set and fine-tuned on k shots in each metatest task, and (2) randomly initialized U-Nets trained independently from scratch on k shots in each meta-test task. To make comparisons fair, we showed the pre-trained model the same amount of data as seen by MAML. If MAML was meta-trained on m support tiles and m query tiles and adapted using k shots, the baseline U-Net was pre-trained on 2m tiles per image and fine-tuned on k shots per metatest tile, and the randomly initialized model was trained on k shots. The U-Net architecture was shared among MAML and both baselines.

**Evaluation.** The performance of all models was evaluated on the query tiles of an unseen meta-test set of images. The location of the k shots for each meta-test image was sampled at random from its support set and fixed across all models for direct comparison. The models were evaluated by means of pixel-wise accuracy and the mean intersection over union (mIoU) score across the meta-test queries. For elaboration on the formula used to compute mIoU, please refer to the DeepGlobe publication [7].

**Random Meta-Dataset Split Results.** When the metadatasets were randomly split, the pre-trained model performed better than MAML and the randomly initialized model. This was especially true at smaller meta-training set sizes (Fig. 6b). In other words, MAML requires a large set of meta-training tasks in order to perform well on new tasks. As the number of shots seen by the meta-learner increases, MAML catches up to the pre-trained model (Fig. 6c). In these experiments, we did not observe fine-tuning of the pre-trained model to improve its performance.

Figure 7a visualizes the predictions of MAML and the baselines on 1-shot learning for two images: one where MAML performs well and one where it fails. MAML appears heavily influenced by the choice of the 1 shot, while the pre-trained model is biased toward predicting agriculture (the most common class). The model trained from scratch is even more heavily influenced by the choice of the 1 shot, as this is the only data it sees during training.

The success of pre-training can be attributed to the complete overlap of meta-train and meta-test distributions, seen in Fig. 6a. In the setting where p(X, y) are identical in the source domain and target domain, a model trained on the source domain transfers perfectly to the target domain. These results also expose MAML’s weaknesses when metatrain size is small: it is not able to retain information about land cover types as effectively as a straightforward supervised model.

**Clustered Meta-Dataset Split Results.** When the metadatasets were split along clusters, the meta-train and metatest distributions overlapped less (Fig. 6a) but could still be considered to arise from the same data-generating distribution. Whereas the meta-train set contains mostly agriculture pixels, the meta-test set contains predominantly forest. Figures 6b and 6c show, first and foremost, that this meta-test set is more difficult than the randomly split meta-test set for all three models. However, MAML is able to adapt to this distributional shift more successfully than the pre-trained model. Example segmentations shown in Fig. 7b reveal that MAML’s flexibility to adaptation can again be both helpful and detrimental: helpful when the 1 shot is representative of the image, but detrimental when it is not. We see that the pre-trained model carries its bias toward agriculture into its meta-test set predictions, whereas MAML does not appear to retain a strong enough prior to recognize agriculture without being provided a shot containing that class.

## Visualization of Model Adaptation
In the introduction and Fig. 1, we showed the regional diversity of representations on the Earth’s surface using PCA on pre-trained VGG-16 image features. In Section 3 and Fig. 2, we assumed that a neural network would achieve optimal performance with a different set of weights φ∗ foreach geographic region. In this experiment, we empirically confirmed this hypothesis with two evaluations on meta-test regions of the Sen12MS dataset. In Section 6.3.1, we visualize the adapted MAML weights for two distinct geographies. Then in Section 6.3.2 we compare the loss surfaces of a MAML-trained and a pre-trained model along one adaptation trajectory. The two evaluations are meant to provide the reader with some intuition of what MAML is doing in different regions and how this differs from pre-training.

### Region-wise Adaptation
We studied the adaptation of MAML-model parameters θ trained on 2-shot 4-way tasks of Section 6.1. We sampled 1000 1-shot 4-way classification tasks from the meta-test regions for the four most common classes (forests, grassland, savanna, urban) and split these into a support and query partition at ratio of 4:1. For each training task, we evaluated the gradient and adapted the model using gradient descent with step size 0.75 to new parameters for each task φτ . We visualized this adaptation by flattening all model parameters to a 231 818-dimensional vector and using PCA to map the parameters to the first two principal components. We colored this embedding by region and drew lines from the initial weights θ to the adapted task-specific weights φτ in Fig. 8. The adapted model-weights differ from region to region in embedding space, as can be seen in the examples of Poland and South Sudan. This empirically shows that a different set of model parameters is optimal for two different regions.

### Loss Surface along Support Gradient
Next, for four example query tasks, we evaluated the loss along a line from the initial parameters θ to task-adapted parameters φ with the MAML-trained model and the pretrained model. For this evaluation, we selected one support set and four query sets from the same region and season. The gradient g was evaluated on the support set, and different model weights φα were obtained along the gradient direction using φα = θ + αg with different step sizes in [0, 1], αpre in [0, 0.15] proportional to the optimal step sizes for MAML and pretrained model. We calculated the query loss using the model fφα for each of the four queries at different step sizes α. This draws a onedimensional slice of the loss-surface along the gradient direction determined by the support set. In Fig. 8b, we show this loss surface for the MAML-trained model and the pretrained classification model. Without adaptation, at α = 0, the MAML-trained model evaluates a high loss compared to the pre-trained model. This is consistent with the comparatively poor zero-shot results from Fig. 5. With increasing step size, however, we observe that the MAML loss decreases consistently while the pre-trained loss remains similar or increased for larger step sizes. The MAML-trained model achieves low loss in a large range of step sizes from 0.1 to 1 for all query sets, while a narrow range of step sizes between 0 and 0.05 lead to better accuracies on some tasks from the pre-trained model initialization.

In general, the loss surface of the MAML-trained model follows a convex curve for all of the test examples, while the loss surface of the pre-trained model is non-convex with local minima. This experiment illustrates the difference between meta-learning and pre-training: the two methods lead to very different model parameters. The loss surface of a meta-learned model is smooth and convex in the gradient direction of a novel task — in other words, when the MAML algorithm optimizes for an initialization θ that can adapt well to new tasks, it seeks out these smooth, convex regions in the loss landscape. By contrast, for the pre-trained model, there are cases like query sets 2 and 4 in which it appears beneficial not to adapt to the specific task’s region.

# Discussion and Conclusion
In this work, we evaluated the model-agnostic metalearning (MAML) algorithm for few-shot problems in land cover classification to adapt deep learning models to individual regions with few data examples. Existing models use regular gradient descent to pre-train a model on a large body of data and use this pre-trained model as an initialization for datasets with fewer examples. We compared these two approaches on land cover classification on the Sen12MS dataset of optical and radar images from globally distributed regions and the DeepGlobe dataset with very high-resolution imagery in few regions. The results on Sen12MS in Section 6.1 demonstrate that MAMLoptimization can outperform regular gradient descent and pre-training of models when the dataset includes a distinct regional diversity. The DeepGlobe results in Section 6.2 illustrate the advantage MAML offers when the source domain differs from the target domain in transfer learning but also highlight MAML’s weaknesses in retaining prior knowledge and under-performing in ideal (identical source and target domain) settings. In Section 6.3, we evaluated the loss surfaces for pre-trained and MAML-trained models and showed that the loss surface was more convex for MAML-trained models when adapting to new unseen data.

We believe that the meta-learning framework can lead deep learning in Earth observation to a new direction: away from finding incrementally better model architectures for specific use-cases and toward unifying strategies that more closely reflect the reality on the Earth’s surface. Much work remains to be done to improve MAML performance by retaining stronger priors on land cover classes, as well as to explore other meta-learning paradigms (e.g. prototypical networks).









