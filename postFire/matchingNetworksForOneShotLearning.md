# Matching Networks for One Shot Learning

# Abstract
Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.

# Introduction
Humans learn new concepts with very little supervision – e.g. a child can generalize the concept of “giraffe” from a single picture in a book – yet our best deep learning systems need hundreds or thousands of examples. This motivates the setting we are interested in: “one-shot” learning, which consists of learning a class from a single labelled example.

Deep learning has made major advances in areas such as speech, vision and language, but is notorious for requiring large datasets. Data augmentation and regularization techniques alleviate overfitting in low data regimes, but do not solve it. Furthermore, learning is still slow and based on large datasets, requiring many weight updates using stochastic gradient descent. This, in our view, is mostly due to the parametric aspect of the model, in which training examples need to be slowly learnt by the model into its parameters.

In contrast, many non-parametric models allow novel examples to be rapidly assimilated, whilst not suffering from catastrophic forgetting. Some models in this family (e.g., nearest neighbors) do not require any training but performance depends on the chosen metric. Previous work on metric learning in non-parametric setups has been influential on our model, and we aim to incorporate the best characteristics from both parametric and non-parametric models – namely, rapid acquisition of new examples while providing excellent generalisation from common examples.

The novelty of our work is twofold: at the modeling level, and at the training procedure. We propose Matching Nets (MN), a neural network which uses recent advances in attention and memory that enable rapid learning. Secondly, our training procedure is based on a simple machine learning principle: test and train conditions must match. Thus to train our network to do rapid learning, we train it by showing only a few examples per class, switching the task from minibatch to minibatch, much like how it will be tested when presented with a few examples of a new task.

Besides our contributions in defining a model and training criterion amenable for one-shot learning, we contribute by the definition of tasks that can be used to benchmark other approaches on both ImageNet and small scale language modeling. We hope that our results will encourage others to work on this challenging problem.

We organized the paper by first defining and explaining our model whilst linking its several components to related work. Then in the following section we briefly elaborate on some of the related work to the task and our model. In Section 4 we describe both our general setup and the experiments we performed, demonstrating strong results on one-shot learning on a variety of tasks and setups.


# Related Work
## Memory Augmented Neural Networks
A recent surge of models which go beyond “static” classification of fixed vectors onto their classes has reshaped current research and industrial applications alike. This is most notable in the massive adoption of LSTMs in a variety of tasks such as speech, translation or learning programs. A key component which allowed for more expressive models was the introduction of “content” based attention in [2], and “computer-like” architectures such as the Neural Turing Machine or Memory Networks. Our work takes the metalearning paradigm of [21], where an LSTM learnt to learn quickly from data presented sequentially, but we treat the data as a set. The one-shot learning task we defined on the Penn Treebank relates to evaluation techniques and models presented in 6, and we discuss this in Section 4.

## Metric Learning
As discussed in Section 2, there are many links between content based attention, kernel based nearest neighbor and metric learning. The most relevant work is Neighborhood Component Analysis (NCA), and the follow up non-linear version. The loss is very similar to ours, except we use the whole support set S instead of pair-wise comparisons which is more amenable to one-shot learning. Follow-up work in the form of deep convolutional siamese networks included much more powerful non-linear mappings. Other losses which include the notion of a set (but use less powerful metrics) were proposed in [28].

Lastly, the work in one-shot learning in [14] was inspirational and also provided us with the invaluable Omniglot dataset – referred to as the “transpose” of MNIST. Other works used zero-shot learning on ImageNet. However, there is not much one-shot literature on ImageNet, which we hope to amend via our benchmark and task definitions in the following section.

# Experiments
In this section we describe the results of many experiments, comparing our Matching Networks model against strong baselines. All of our experiments revolve around the same basic task: an N-way k-shot learning task. Each method is providing with a set of k labelled examples from each of N classes that have not previously been trained upon. The task is then to classify a disjoint batch of unlabelled examples into one of these N classes. Thus random performance on this task stands at 1/N. We compared a number of alternative models, as baselines, to Matching Networks.

## Image Classification Results
For vision problems, we considered four kinds of baselines: matching on raw pixels, matching on discriminative features from a state-of-the-art classifier (Baseline Classifier), MANN, and our reimplementation of the Convolutional Siamese Net. The baseline classifier was trained to classify an image into one of the original classes present in the training data set, but excluding the N classes so as not to give it an unfair advantage (i.e., trained to classify classes in 6=L0). We then took this network and used the features from the last layer (before the softmax) for nearest neighbour matching, a strategy commonly used in computer vision which has achieved excellent results across many tasks. Following [11], the convolutional siamese nets were trained on a same-or-different task of the original training data set and then the last layer was used for nearest neighbour matching.

We also tried further fine tuning the features using only the support set S0 sampled from L0. This yields massive overfitting, but given that our networks are highly regularized, can yield extra gains. Note that, even when fine tuning, the setup is still one-shot, as only a single example per class from L0 is used.

### Omniglot
Omniglot consists of 1623 characters from 50 different alphabets. Each of these was hand drawn by 20 different people. The large number of classes (characters) with relatively few data per class (20), makes this an ideal data set for testing small-scale one-shot classification. The N-way Omniglot task setup is as follows: pick N unseen character classes, independent of alphabet, as L. Provide the model with one drawing of each of the N characters as S∼L and a batch B∼L. Following [21], we augmented the data set with random rotations by multiples of 90 degrees and used 1200 characters for training, and the remaining character classes for evaluation.

We used a simple yet powerful CNN as the embedding function – consisting of a stack of modules, each of which is a 3×3 convolution with 64 filters followed by batch normalization, a Relu non-linearity and 2 × 2 max-pooling. We resized all the images to 28×28 so that, when we stack 4 modules, the resulting feature map is 1×1×64, resulting in our embedding function f(x). A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier.

Results comparing the baselines to our model on Omniglot are shown in Table 1. For both 1-shot and 5-shot, 5-way and 20-way, our model outperforms the baselines. There are no major surprises in these results: using more examples for k-shot classification helps all models, and 5-way is easier than 20-way. We note that the Baseline Classifier improves a bit when fine tuning on S0, and using cosine distance versus training a small softmax from the small training set (thus requiring fine tuning) also performs well. Siamese nets fare well versus our Matching Nets when using 5 examples per class, but their performance degrades rapidly in one-shot. Fully Conditional Embeddings (FCE) did not seem to help much and were left out of the table due to space constraints.

Like the authors in [11], we also test our method trained on Omniglot on a completely disjoint task – one-shot, 10 way MNIST classification. The Baseline Classifier does about 63% accuracy whereas (as reported in their paper) the Siamese Nets do 70%. Our model achieves 72%.

# Conclusion
In this paper we introduced Matching Networks, a new neural architecture that, by way of its corresponding training regime, is capable of state-of-the-art performance on a variety of one-shot classification tasks. There are a few key insights in this work. Firstly, one-shot learning is much easier if you train the network to do one-shot learning. Secondly, non-parametric structures in a neural network make it easier for networks to remember and adapt to new training sets in the same tasks. Combining these observations together yields Matching Networks. Further, we have defined new one-shot tasks on ImageNet, a reduced version of ImageNet (for rapid experimentation), and a language modeling task. An obvious drawback of our model is the fact that, as the support set S grows in size, the computation for each gradient update becomes more expensive. Although there are sparse and sampling-based methods to alleviate this, much of our future efforts will concentrate around this limitation. Further, as exemplified in the ImageNet dogs subtask, when the label distribution has obvious biases (such as being fine grained), our model suffers. We feel this is an area with exciting challenges which we hope to keep improving in future work.

# Acknowledgements
We would like to thank Nal Kalchbrenner for brainstorming around the design of the function g, and Sander Dieleman and Sergio Guadarrama for their help setting up ImageNet. We would also like thank Simon Osindero for useful discussions around the tasks discussed in this paper, and Theophane Weber and Remi Munos for following some early developments. Karen Simonyan and David Silver helped with the manuscript, as well as many at Google DeepMind. Thanks also to Geoff Hinton and Alex Toshev for discussions about our results.
